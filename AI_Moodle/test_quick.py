#!/usr/bin/env python3
"""
‚ö° QUICK TEST - Test nhanh ch·ª©c nƒÉng sinh c√¢u h·ªèi & ƒë√°nh gi√°
D√πng khi mu·ªën test m√† kh√¥ng c·∫ßn ch·∫°y full pipeline
"""

import chromadb
import ollama
import time

# K·∫øt n·ªëi ChromaDB
client = chromadb.PersistentClient(path="./db_moodle")
collection = client.get_collection(name="giao_trinh_c")

def evaluate_score(context_or_topic, generated_text, eval_type, eval_model="qwen2.5:7b"):
    """ƒê√°nh gi√° 1 ti√™u ch√≠"""
    if eval_type == "faithfulness":
        prompt = f"""ƒê√°nh gi√° (0.0-1.0): c√¢u h·ªèi c√≥ d·ª±a tr√™n context kh√¥ng?
Context: {context_or_topic[:200]}...
C√¢u: {generated_text[:200]}...
Ch·ªâ tr·∫£ l·ªùi s·ªë."""
    elif eval_type == "relevancy":
        prompt = f"""ƒê√°nh gi√° (0.0-1.0): c√¢u h·ªèi c√≥ li√™n quan '{context_or_topic}' kh√¥ng?
C√¢u: {generated_text[:200]}...
Ch·ªâ tr·∫£ l·ªùi s·ªë."""
    else:  # quality
        prompt = f"""ƒê√°nh gi√° (0.0-1.0): ch·∫•t l∆∞·ª£ng, r√µ r√†ng, ƒë√∫ng format?
C√¢u: {generated_text[:200]}...
Ch·ªâ tr·∫£ l·ªùi s·ªë."""
    
    try:
        resp = ollama.generate(model=eval_model, prompt=prompt, stream=False)
        text = resp['response'].strip()
        score = float(''.join(c for c in text if c.isdigit() or c == '.')[:3])
        return min(1.0, max(0.0, score / 10 if score > 1 else score))
    except:
        return 0.5

def test_question(topic, model, question_type):
    """Test sinh & ƒë√°nh gi√° 1 c√¢u h·ªèi"""
    print(f"\nüß™ {question_type.ljust(15)} | {model}")
    print("-" * 50)
    
    try:
        # L·∫•y context
        results = collection.query(query_texts=[topic], n_results=1)
        context = results['documents'][0][0]
        
        # Sinh c√¢u h·ªèi
        prompt = f"Ng·ªØ c·∫£nh: {context}\nT·∫°o 1 c√¢u tr·∫Øc nghi·ªám {question_type} format Aiken A) B) C) D) ANSWER:"
        response = ollama.generate(model=model, prompt=prompt, stream=False)
        question = response['response']
        
        # ƒê√°nh gi√° 3 metrics
        faith = evaluate_score(context, question, "faithfulness")
        time.sleep(0.3)
        relev = evaluate_score(topic, question, "relevancy")
        time.sleep(0.3)
        qual = evaluate_score("", question, "quality")
        
        avg = (faith + relev + qual) / 3
        
        # In k·∫øt qu·∫£
        print(f"C√¢u: {question[:60]}...")
        print(f"Scores: Faith={faith:.2f} | Relev={relev:.2f} | Quality={qual:.2f} | Avg={avg:.2f}")
        print(f"Status: {'‚úÖ Pass' if avg >= 0.35 else '‚ö†Ô∏è  Low'}")
        
        return avg
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return 0

def main():
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë    ‚ö° QUICK TEST - SINH C√ÇU H·ªéI & ƒê√ÅNH GI√Å   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    tests = [
        ("Gi·∫£i thu·∫≠t l√† g√¨", "qwen2.5:7b", "L√Ω thuy·∫øt"),
        ("Ho√°n ƒë·ªïi hai bi·∫øn", "llama3.1:8b", "Logic"),
        ("C·∫•u tr√∫c v√≤ng l·∫∑p for", "deepseek-coder-v2", "Code C"),
    ]
    
    scores = []
    for topic, model, qtype in tests:
        score = test_question(topic, model, qtype)
        scores.append((qtype, score))
        time.sleep(1)
    
    # Summary
    print(f"\n{'='*50}")
    print("üìä T√ìM T·∫ÆT")
    print(f"{'='*50}")
    for qtype, score in scores:
        status = "‚úÖ" if score >= 0.35 else "‚ö†Ô∏è "
        print(f"  {status} {qtype.ljust(15)}: {score:.2f}")

if __name__ == "__main__":
    main()
