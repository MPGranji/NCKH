#!/usr/bin/env python3
"""
ğŸ” TEST DEEPSEEK CODE - So sÃ¡nh 3 model trÃªn cÃ¢u há»i CODE
So sÃ¡nh: deepseek-coder-v2 vs qwen2.5:7b vs llama3.1:8b
"""

import chromadb
import ollama
import time

# Káº¿t ná»‘i ChromaDB
client = chromadb.PersistentClient(path="./db_moodle")
collection = client.get_collection(name="giao_trinh_c")

def evaluate_score(context_or_topic, generated_text, eval_type, eval_model="qwen2.5:7b"):
    """ÄÃ¡nh giÃ¡ 1 tiÃªu chÃ­"""
    if eval_type == "faithfulness":
        prompt = f"""ÄÃ¡nh giÃ¡ (0.0-1.0): cÃ¢u há»i cÃ³ dá»±a trÃªn context khÃ´ng?
Context: {context_or_topic[:200]}...
CÃ¢u: {generated_text[:200]}...
Chá»‰ tráº£ lá»i sá»‘."""
    elif eval_type == "relevancy":
        prompt = f"""ÄÃ¡nh giÃ¡ (0.0-1.0): cÃ¢u há»i cÃ³ liÃªn quan '{context_or_topic}' khÃ´ng?
CÃ¢u: {generated_text[:200]}...
Chá»‰ tráº£ lá»i sá»‘."""
    else:  # quality
        prompt = f"""ÄÃ¡nh giÃ¡ (0.0-1.0): cháº¥t lÆ°á»£ng, rÃµ rÃ ng, Ä‘Ãºng format?
CÃ¢u: {generated_text[:200]}...
Chá»‰ tráº£ lá»i sá»‘."""
    
    try:
        resp = ollama.generate(model=eval_model, prompt=prompt, stream=False)
        text = resp['response'].strip()
        score = float(''.join(c for c in text if c.isdigit() or c == '.')[:3])
        return min(1.0, max(0.0, score / 10 if score > 1 else score))
    except:
        return 0.5

def generate_code_question(topic, model):
    """Sinh 1 cÃ¢u há»i code"""
    try:
        # Láº¥y context
        results = collection.query(query_texts=[topic], n_results=1)
        context = results['documents'][0][0]
        
        # Sinh cÃ¢u há»i code
        prompt = f"Ngá»¯ cáº£nh: {context}\nTáº¡o 1 cÃ¢u tráº¯c nghiá»‡m CODE C format Aiken A) B) C) D) ANSWER:"
        response = ollama.generate(model=model, prompt=prompt, stream=False)
        question = response['response']
        
        # ÄÃ¡nh giÃ¡ 3 metrics
        faith = evaluate_score(context, question, "faithfulness")
        time.sleep(0.2)
        relev = evaluate_score(topic, question, "relevancy")
        time.sleep(0.2)
        qual = evaluate_score("", question, "quality")
        
        avg = (faith + relev + qual) / 3
        
        return {
            'question': question,
            'faith': faith,
            'relev': relev,
            'qual': qual,
            'avg': avg,
            'model': model
        }
    except Exception as e:
        print(f"âŒ Error ({model}): {str(e)}")
        return None

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ğŸ” TEST DEEPSEEK CODE - SO SÃNH 3 MODEL               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    topic = "Cáº¥u trÃºc vÃ²ng láº·p for"
    qtype = "Code C"
    
    models = [
        "deepseek-coder-v2",
        "qwen2.5:7b",
        "llama3.1:8b"
    ]
    
    print(f"\nğŸ“Œ Topic: {topic}")
    print(f"ğŸ“Œ Loáº¡i cÃ¢u: {qtype}")
    print(f"\nğŸ”„ Sinh cÃ¢u há»i tá»« 3 model...\n")
    
    results = []
    colors = ["ğŸŸ¦", "ğŸŸ¨", "ğŸŸ©"]
    
    for i, model in enumerate(models):
        print(f"   {colors[i]} {model}...", end=" ", flush=True)
        result = generate_code_question(topic, model)
        if result:
            results.append(result)
            print("âœ…")
        else:
            print("âŒ")
        time.sleep(1)
    
    # So sÃ¡nh
    print("\n" + "="*70)
    print("ğŸ“Š Káº¾T QUáº¢ SO SÃNH CHI TIáº¾T")
    print("="*70 + "\n")
    
    for i, result in enumerate(results):
        print(f"{colors[i]} {result['model'].ljust(20)}")
        print(f"   CÃ¢u: {result['question'][:90]}...")
        print(f"   â”œâ”€ Faithfulness: {result['faith']:.2f}")
        print(f"   â”œâ”€ Relevancy:    {result['relev']:.2f}")
        print(f"   â”œâ”€ Quality:      {result['qual']:.2f}")
        print(f"   â””â”€ â­ TRUNG BÃŒNH: {result['avg']:.2f} {'âœ… Pass' if result['avg'] >= 0.35 else 'âš ï¸  Low'}")
        print()
    
    # Xáº¿p háº¡ng
    print("="*70)
    print("ğŸ† Báº¢NG Xáº¾P Háº NG")
    print("="*70 + "\n")
    
    sorted_results = sorted(results, key=lambda x: x['avg'], reverse=True)
    
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    for i, result in enumerate(sorted_results):
        print(f"   {medals[i]} {i+1}. {result['model'].ljust(25)} - {result['avg']:.2f}")
    
    print("\n" + "="*70)
    print("ğŸ’¬ NHáº¬N XÃ‰T")
    print("="*70)
    
    best = sorted_results[0]
    worst = sorted_results[-1]
    diff = best['avg'] - worst['avg']
    
    print(f"""
   âœ… Model tá»‘t nháº¥t: {best['model']}
      Äiá»ƒm: {best['avg']:.2f}
   
   âš ï¸  Model yáº¿u nháº¥t: {worst['model']}
      Äiá»ƒm: {worst['avg']:.2f}
   
   ğŸ“ˆ ChÃªnh lá»‡ch: {diff:.2f} Ä‘iá»ƒm
    """)

if __name__ == "__main__":
    main()
