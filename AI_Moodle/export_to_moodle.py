import chromadb
import ollama
import time
import re
import xml.etree.ElementTree as ET
from xml.dom import minidom

# K·∫øt n·ªëi ChromaDB
client = chromadb.PersistentClient(path="./db_moodle")
collection = client.get_collection(name="giao_trinh_c")

def evaluate_faithfulness(context, generated_text, eval_model="qwen2.5:7b"):
    """ƒê√°nh gi√° ƒë·ªô trung th·ª±c"""
    prompt = f"""ƒê√°nh gi√° xem c√¢u h·ªèi sau c√≥ d·ª±a tr√™n th√¥ng tin t·ª´ context cung c·∫•p kh√¥ng.
Context: {context[:300]}...
C√¢u h·ªèi: {generated_text[:300]}...

Tr·∫£ l·ªùi b·∫±ng m·ªôt s·ªë t·ª´ 0.0 ƒë·∫øn 1.0.
Ch·ªâ tr·∫£ l·ªùi s·ªë th√¥i."""
    
    try:
        response = ollama.generate(model=eval_model, prompt=prompt, stream=False)
        score_text = response['response'].strip()
        score = float(''.join(c for c in score_text if c.isdigit() or c == '.')[:3])
        return min(1.0, max(0.0, score / 10 if score > 1 else score))
    except:
        return 0.5

def evaluate_relevancy(topic, generated_text, eval_model="llama3.1:8b"):
    """ƒê√°nh gi√° ƒë·ªô li√™n quan"""
    prompt = f"""ƒê√°nh gi√° xem c√¢u h·ªèi sau c√≥ li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ '{topic}' kh√¥ng.
C√¢u h·ªèi: {generated_text[:300]}...

Tr·∫£ l·ªùi b·∫±ng m·ªôt s·ªë t·ª´ 0.0 ƒë·∫øn 1.0.
Ch·ªâ tr·∫£ l·ªùi s·ªë th√¥i."""
    
    try:
        response = ollama.generate(model=eval_model, prompt=prompt, stream=False)
        score_text = response['response'].strip()
        score = float(''.join(c for c in score_text if c.isdigit() or c == '.')[:3])
        return min(1.0, max(0.0, score / 10 if score > 1 else score))
    except:
        return 0.5

def evaluate_quality(generated_text, question_type, eval_model="qwen2.5:7b"):
    """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u h·ªèi"""
    prompt = f"""ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c√¢u h·ªèi tr·∫Øc nghi·ªám ({question_type}) sau:
{generated_text[:300]}...

Ti√™u ch√≠: R√µ r√†ng, ƒë√∫ng format, ƒë√°p √°n h·ª£p l√Ω.
Tr·∫£ l·ªùi b·∫±ng s·ªë t·ª´ 0.0 ƒë·∫øn 1.0.
Ch·ªâ tr·∫£ l·ªùi s·ªë th√¥i."""
    
    try:
        response = ollama.generate(model=eval_model, prompt=prompt, stream=False)
        score_text = response['response'].strip()
        score = float(''.join(c for c in score_text if c.isdigit() or c == '.')[:3])
        return min(1.0, max(0.0, score / 10 if score > 1 else score))
    except:
        return 0.5

def extract_aiken_format(question_text):
    """
    Tr√≠ch xu·∫•t format Aiken t·ª´ text sinh b·ªüi LLM
    Parse linh ho·∫°t: t√¨m c√¢u h·ªèi (d√≤ng "C√¢u h·ªèi: ..."), 4 ƒë√°p √°n, v√† ANSWER
    """
    import re
    
    lines = question_text.strip().split('\n')
    
    question = None
    options = {}
    answer = None
    
    # T√¨m d√≤ng c√¢u h·ªèi - d√≤ng b·∫Øt ƒë·∫ßu v·ªõi "C√¢u h·ªèi:" ho·∫∑c "Question:"
    for idx, line in enumerate(lines):
        line_clean = line.strip()
        
        # Pattern: "C√¢u h·ªèi: ..." ho·∫∑c "Question: ..."
        if re.match(r'^(C√¢u h·ªèi|Question|c√¢u h·ªèi):\s*', line_clean):
            # Extract ph·∫ßn sau "C√¢u h·ªèi:"
            question = re.sub(r'^(C√¢u h·ªèi|Question|c√¢u h·ªèi):\s*', '', line_clean)
            break
        # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c, l·∫•y d√≤ng ƒë·∫ßu ti√™n > 10 k√Ω t·ª±
        elif len(line_clean) > 10 and not re.match(r'^[A-D]\)', line_clean) and '?' in line_clean:
            question = line_clean
            break
    
    # Fallback: n·∫øu v·∫´n kh√¥ng t√¨m ƒë∆∞·ª£c, l·∫•y d√≤ng ƒë·∫ßu ti√™n + 100 k√Ω t·ª±
    if not question:
        for line in lines:
            line = line.strip()
            if len(line) > 10 and not re.match(r'^[A-D]\)', line) and not line.startswith('['):
                question = line[:100]
                break
    
    if not question:
        question = "C√¢u h·ªèi tr·∫Øc nghi·ªám"
    
    # T√¨m c√°c ƒë√°p √°n - pattern: A) ..., A. ..., A: ...
    for line in lines:
        line = line.strip()
        
        for opt_char in ['A', 'B', 'C', 'D']:
            # Pattern 1: A) ..., A. ..., A: ...
            if re.match(rf'^{opt_char}[\)\.:\s]', line):
                text = re.sub(rf'^{opt_char}[\)\.:\s]+', '', line).strip()
                text = re.sub(r'^\[.*?\]\s*', '', text)  # X√≥a checkbox
                text = re.sub(r'^\(.*?\)\s*', '', text)  # X√≥a parentheses
                if text and len(text) > 2:
                    options[opt_char] = text
            
            # Pattern 2: - [A] ... ho·∫∑c - [ ] ...
            elif re.match(rf'^-\s*\[.*?{opt_char}.*?\]', line):
                text = re.sub(r'^-\s*\[.*?\]', '', line).strip()
                if text:
                    options[opt_char] = text
    
    # T√¨m ƒë√°p √°n ƒë√∫ng - d√≤ng c√≥ ANSWER:
    for line in lines:
        line_upper = line.upper()
        if 'ANSWER' in line_upper:
            for opt in ['A', 'B', 'C', 'D']:
                if opt in line:
                    answer = opt
                    break
        elif '[X]' in line_upper or '[x]' in line:
            for opt in ['A', 'B', 'C', 'D']:
                if f'{opt}' in line or f'{opt})' in line:
                    answer = opt
                    break
    
    # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c answer, l·∫•y option ƒë·∫ßu ti√™n
    if not answer:
        answer = list(options.keys())[0] if options else 'A'
    
    # N·∫øu thi·∫øu 4 options, t·∫°o fallback
    if len(options) < 4:
        fallback = [
            "Kh√¥ng ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi",
            "C·∫ßn th√™m d·ªØ li·ªáu",
            "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh",
            "ƒê√°p √°n kh√°c",
        ]
        
        for opt in ['A', 'B', 'C', 'D']:
            if opt not in options:
                idx = ord(opt) - ord('A')
                options[opt] = fallback[idx] if idx < len(fallback) else f"T√πy ch·ªçn {opt}"
    
    return {
        'question': question,
        'options': options,
        'answer': answer
    }

def generate_questions(topic, model_name, question_type, eval_threshold=0.4):
    """Sinh c√¢u h·ªèi t·ª´ topic v√† ƒë√°nh gi√°"""
    try:
        results = collection.query(query_texts=[topic], n_results=1)
        if not results['documents'] or not results['documents'][0]:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho topic: {topic}")
            return None
        
        context = results['documents'][0][0]
        
        # Prompt sinh c√¢u h·ªèi
        if question_type == "Code C":
            prompt = f"""B·∫°n l√† gi√°o vi√™n l·∫≠p tr√¨nh ƒëang t·∫°o c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ code C.
D∆∞·ªõi ƒë√¢y l√† ng·ªØ c·∫£nh t·ª´ gi√°o tr√¨nh:
{context[:300]}

T·∫°o 1 c√¢u h·ªèi tr·∫Øc nghi·ªám theo format n√†y CH√çNH X√ÅC:

C√¢u h·ªèi: [VI·∫æT C√ÇU H·ªéI C·ª§ TH·ªÇ V·ªÄ CODE - kh√¥ng vi·∫øt "C√¢u h·ªèi?"]
A) [ƒê√ÅP √ÅN A]
B) [ƒê√ÅP √ÅN B]
C) [ƒê√ÅP √ÅN C]
D) [ƒê√ÅP √ÅN D]
ANSWER: A"""
        elif question_type == "L·ªói sai Code":
            prompt = f"""B·∫°n l√† gi√°o vi√™n l·∫≠p tr√¨nh ƒëang t·∫°o c√¢u h·ªèi v·ªÅ l·ªói trong code C.
Ng·ªØ c·∫£nh: {context[:300]}

T·∫°o 1 c√¢u h·ªèi theo format CH√çNH X√ÅC:

C√¢u h·ªèi: [VI·∫æT C√ÇU H·ªéI C·ª§ TH·ªÄ V·ªÄ L·ªñI CODE]
A) [ƒê√ÅP √ÅN A]
B) [ƒê√ÅP √ÅN B]
C) [ƒê√ÅP √ÅN C]
D) [ƒê√ÅP √ÅN D]
ANSWER: B"""
        elif question_type == "Logic":
            prompt = f"""B·∫°n l√† gi√°o vi√™n ƒëang t·∫°o c√¢u h·ªèi v·ªÅ logic l·∫≠p tr√¨nh.
Ng·ªØ c·∫£nh: {context[:300]}

T·∫°o 1 c√¢u h·ªèi theo format CH√çNH X√ÅC:

C√¢u h·ªèi: [VI·∫æT C√ÇU H·ªéI C·ª§ TH·ªÇ V·ªÄ LOGIC/THU·∫¨T TO√ÅN]
A) [ƒê√ÅP √ÅN A]
B) [ƒê√ÅP √ÅN B]
C) [ƒê√ÅP √ÅN C]
D) [ƒê√ÅP √ÅN D]
ANSWER: C"""
        else:  # L√Ω thuy·∫øt
            prompt = f"""B·∫°n l√† gi√°o vi√™n l·∫≠p tr√¨nh t·∫°o c√¢u h·ªèi l√Ω thuy·∫øt.
Ng·ªØ c·∫£nh: {context[:300]}

T·∫°o 1 c√¢u h·ªèi theo format CH√çNH X√ÅC:

C√¢u h·ªèi: [VI·∫æT C√ÇU H·ªéI C·ª§ TH·ªÄ V·ªÄ KH√ÅI NI·ªÜM - kh√¥ng vi·∫øt "C√¢u h·ªèi?"]
A) [ƒê√ÅP √ÅN A - ƒê√öNG]
B) [ƒê√ÅP √ÅN B - SAI]
C) [ƒê√ÅP √ÅN C - SAI]
D) [ƒê√ÅP √ÅN D - SAI]
ANSWER: A"""
        
        # Sinh c√¢u h·ªèi
        response = ollama.generate(model=model_name, prompt=prompt, stream=False)
        generated_text = response['response']
        
        # ƒê√°nh gi√°
        faithfulness = evaluate_faithfulness(context, generated_text)
        time.sleep(0.5)
        relevancy = evaluate_relevancy(topic, generated_text)
        time.sleep(0.5)
        quality = evaluate_quality(generated_text, question_type)
        
        avg_score = (faithfulness + relevancy + quality) / 3
        
        if avg_score < eval_threshold:
            print(f"‚ö†Ô∏è  C√¢u h·ªèi kh√¥ng ƒë·∫°t chu·∫©n (score: {avg_score:.2f} < {eval_threshold})")
            return None
        
        # Parse format Aiken
        parsed = extract_aiken_format(generated_text)
        if not parsed:
            print(f"‚ùå Kh√¥ng th·ªÉ parse format Aiken")
            return None
        
        return {
            'topic': topic,
            'type': question_type,
            'model': model_name,
            'question': parsed['question'],
            'options': parsed['options'],
            'answer': parsed['answer'],
            'scores': {
                'faithfulness': faithfulness,
                'relevancy': relevancy,
                'quality': quality,
                'average': avg_score
            },
            'raw_response': generated_text
        }
        
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        return None

def create_moodle_xml(questions):
    """
    T·∫°o file XML theo format Moodle
    Format: https://docs.moodle.org/402/en/Aiken_format
    """
    quiz = ET.Element('quiz')
    
    for idx, q in enumerate(questions, 1):
        question = ET.SubElement(quiz, 'question', type='multichoice')
        question.set('format', 'aiken')
        
        # Name
        name = ET.SubElement(question, 'name')
        name_text = ET.SubElement(name, 'text')
        name_text.text = f"{q['type']} - Q{idx}"
        
        # Question text
        questiontext = ET.SubElement(question, 'questiontext', format='html')
        text = ET.SubElement(questiontext, 'text')
        text.text = q['question']
        
        # General (metadata)
        general = ET.SubElement(question, 'general')
        
        # Answer options
        answer = ET.SubElement(question, 'answer', fraction='100' if q['answer'] == 'A' else '0')
        answer_text = ET.SubElement(answer, 'text')
        answer_text.text = q['options'].get('A', '')
        
        for opt in ['B', 'C', 'D']:
            answer = ET.SubElement(question, 'answer', fraction='100' if q['answer'] == opt else '0')
            answer_text = ET.SubElement(answer, 'text')
            answer_text.text = q['options'].get(opt, '')
        
        # Shuffle
        shuffle = ET.SubElement(question, 'shuffle')
        shuffle.text = '1'
        
        # Single answer
        single = ET.SubElement(question, 'single')
        single.text = 'true'
    
    # Pretty print with proper XML declaration
    xml_str = ET.tostring(quiz, encoding='unicode')
    dom = minidom.parseString(xml_str)
    xml_lines = dom.toprettyxml(indent="  ").split('\n')
    
    # Th√™m XML declaration
    result_lines = ['<?xml version="1.0" encoding="UTF-8"?>']
    
    # B·ªè 2 d√≤ng ƒë·∫ßu c·ªßa dom output (declaration + blank), th√™m ph·∫ßn c√≤n l·∫°i
    for line in xml_lines[2:]:
        if line.strip():  # B·ªè d√≤ng tr·ªëng
            result_lines.append(line)
    
    return '\n'.join(result_lines)

def export_to_file(questions, output_file="questions_export.xml"):
    """Export c√¢u h·ªèi sang file XML"""
    if not questions:
        print("‚ùå Kh√¥ng c√≥ c√¢u h·ªèi ƒë·ªÉ export")
        return False
    
    xml_content = create_moodle_xml(questions)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(xml_content)
    
    print(f"‚úÖ ƒê√£ export {len(questions)} c√¢u h·ªèi sang {output_file}")
    return True

if __name__ == "__main__":
    print("üöÄ B·∫ÆT ƒê·∫¶U SINH V √Ä EXPORT C√ÇU H·ªéI\n")
    
    configs = [
        ("Gi·∫£i thu·∫≠t l√† g√¨", "qwen2.5:7b", "L√Ω thuy·∫øt"),
        ("Ho√°n ƒë·ªïi hai bi·∫øn", "llama3.1:8b", "Logic"),
        ("C·∫•u tr√∫c v√≤ng l·∫∑p for", "deepseek-coder-v2", "Code C"),
        ("L·ªói trong code C", "deepseek-coder-v2", "L·ªói sai Code")
    ]
    
    all_questions = []
    for topic, model, qtype in configs:
        print(f"\nüìù Sinh c√¢u h·ªèi: {qtype} | Model: {model}")
        q = generate_questions(topic, model, qtype, eval_threshold=0.35)
        if q:
            all_questions.append(q)
            print(f"‚úÖ C√¢u h·ªèi: {q['question'][:50]}...")
            print(f"   Score: {q['scores']['average']:.2f}")
        time.sleep(2)
    
    # Export
    if all_questions:
        export_to_file(all_questions, "questions_export.xml")
        print(f"\nüìä T·ªïng: {len(all_questions)}/{len(configs)} c√¢u h·ªèi")
